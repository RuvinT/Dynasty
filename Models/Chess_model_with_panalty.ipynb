{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bace62-1424-468b-a755-10cb540a4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "'''\n",
    "# Function to parse PGN file and extract game data\n",
    "def parse_pgn_file(file_path, max_games=5000):\n",
    "    games = []\n",
    "    with open(file_path) as f:\n",
    "        game_counter = 0\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(f)\n",
    "            if game is None or game_counter >= max_games:\n",
    "                break\n",
    "            games.append(game)\n",
    "            game_counter += 1\n",
    "    return games\n",
    "\n",
    "# Load PGN data\n",
    "pgn_file = \"/Users/ruvinjagoda/Desktop/Aka/AIP/Chess_code/2023_full.pgn\"\n",
    "games = parse_pgn_file(pgn_file, max_games=50000)\n",
    "\n",
    "# Check the number of games loaded\n",
    "print(f\"Number of games loaded: {len(games)}\")\n",
    "'''\n",
    "pgn_file = \"/Users/ruvinjagoda/Desktop/Aka/AIP/Chess_code/2023_full.pgn\"\n",
    "# Function to parse PGN file and yield game data in batches\n",
    "def parse_pgn_file_in_batches(file_path, batch_size=5000):\n",
    "    with open(file_path) as f:\n",
    "        while True:\n",
    "            games = []\n",
    "            for _ in range(batch_size):\n",
    "                game = chess.pgn.read_game(f)\n",
    "                if game is None:\n",
    "                    break\n",
    "                games.append(game)\n",
    "            if not games:\n",
    "                break\n",
    "            yield games\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c794c8-4b0c-4959-bd2a-0a818aff91b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cycle 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Step 0/2903, Loss: 118008.3125\n",
      "Step 100/2903, Loss: 126006.9375\n",
      "Step 200/2903, Loss: 116006.34375\n",
      "Step 300/2903, Loss: 103006.515625\n",
      "Step 400/2903, Loss: 95006.03125\n",
      "Step 500/2903, Loss: 116005.8046875\n",
      "Step 600/2903, Loss: 87005.78125\n",
      "Step 700/2903, Loss: 90006.40625\n",
      "Step 800/2903, Loss: 84005.90625\n",
      "Step 900/2903, Loss: 96005.953125\n",
      "Step 1000/2903, Loss: 77006.359375\n",
      "Step 1100/2903, Loss: 93005.171875\n",
      "Step 1200/2903, Loss: 78006.109375\n",
      "Step 1300/2903, Loss: 92006.2890625\n",
      "Step 1400/2903, Loss: 95006.4765625\n",
      "Step 1500/2903, Loss: 86006.65625\n",
      "Step 1600/2903, Loss: 98006.28125\n",
      "Step 1700/2903, Loss: 84005.875\n",
      "Step 1800/2903, Loss: 78006.03125\n",
      "Step 1900/2903, Loss: 72005.65625\n",
      "Step 2000/2903, Loss: 75006.21875\n",
      "Step 2100/2903, Loss: 86005.9140625\n",
      "Step 2200/2903, Loss: 73005.8828125\n",
      "Step 2300/2903, Loss: 79006.09375\n",
      "Step 2400/2903, Loss: 85005.734375\n",
      "Step 2500/2903, Loss: 83006.03125\n",
      "Step 2600/2903, Loss: 87005.640625\n",
      "Step 2700/2903, Loss: 95006.2265625\n",
      "Step 2800/2903, Loss: 100006.21875\n",
      "Step 2900/2903, Loss: 87005.640625\n",
      "Epoch 2/5\n",
      "Step 0/2903, Loss: 80005.71875\n",
      "Step 100/2903, Loss: 100006.3203125\n",
      "Step 200/2903, Loss: 79005.53125\n",
      "Step 300/2903, Loss: 87006.0078125\n",
      "Step 400/2903, Loss: 77005.375\n",
      "Step 500/2903, Loss: 92005.1875\n",
      "Step 600/2903, Loss: 74004.96875\n",
      "Step 700/2903, Loss: 81005.546875\n",
      "Step 800/2903, Loss: 75005.1796875\n",
      "Step 900/2903, Loss: 77005.2734375\n",
      "Step 1000/2903, Loss: 79005.40625\n",
      "Step 1100/2903, Loss: 66004.21875\n",
      "Step 1200/2903, Loss: 90005.375\n",
      "Step 1300/2903, Loss: 91005.75\n",
      "Step 1400/2903, Loss: 85005.71875\n",
      "Step 1500/2903, Loss: 93006.078125\n",
      "Step 1600/2903, Loss: 86005.5625\n",
      "Step 1700/2903, Loss: 73005.2734375\n",
      "Step 1800/2903, Loss: 75005.359375\n",
      "Step 1900/2903, Loss: 72004.953125\n",
      "Step 2000/2903, Loss: 76005.6171875\n",
      "Step 2100/2903, Loss: 77005.2890625\n",
      "Step 2200/2903, Loss: 81005.0625\n",
      "Step 2300/2903, Loss: 72005.703125\n",
      "Step 2400/2903, Loss: 77005.25\n",
      "Step 2500/2903, Loss: 85005.53125\n",
      "Step 2600/2903, Loss: 89005.21875\n",
      "Step 2700/2903, Loss: 82005.78125\n",
      "Step 2800/2903, Loss: 99005.7421875\n",
      "Step 2900/2903, Loss: 69005.0078125\n",
      "Epoch 3/5\n",
      "Step 0/2903, Loss: 81005.1796875\n",
      "Step 100/2903, Loss: 102005.921875\n",
      "Step 200/2903, Loss: 74004.90625\n",
      "Step 300/2903, Loss: 93005.625\n",
      "Step 400/2903, Loss: 83004.9375\n",
      "Step 500/2903, Loss: 81004.5625\n",
      "Step 600/2903, Loss: 65004.453125\n",
      "Step 700/2903, Loss: 83005.1484375\n",
      "Step 800/2903, Loss: 68004.8515625\n",
      "Step 900/2903, Loss: 78004.9609375\n",
      "Step 1000/2903, Loss: 66004.9296875\n",
      "Step 1100/2903, Loss: 49003.703125\n",
      "Step 1200/2903, Loss: 89004.9375\n",
      "Step 1300/2903, Loss: 85005.5\n",
      "Step 1400/2903, Loss: 87005.421875\n",
      "Step 1500/2903, Loss: 86005.625\n",
      "Step 1600/2903, Loss: 84005.140625\n",
      "Step 1700/2903, Loss: 61004.9453125\n",
      "Step 1800/2903, Loss: 73004.9921875\n",
      "Step 1900/2903, Loss: 67004.515625\n",
      "Step 2000/2903, Loss: 78005.265625\n",
      "Step 2100/2903, Loss: 82005.0078125\n",
      "Step 2200/2903, Loss: 77004.7109375\n",
      "Step 2300/2903, Loss: 69005.34375\n",
      "Step 2400/2903, Loss: 78004.921875\n",
      "Step 2500/2903, Loss: 86005.1875\n",
      "Step 2600/2903, Loss: 81004.890625\n",
      "Step 2700/2903, Loss: 82005.515625\n",
      "Step 2800/2903, Loss: 94005.484375\n",
      "Step 2900/2903, Loss: 69004.765625\n",
      "Epoch 4/5\n",
      "Step 0/2903, Loss: 76004.8984375\n",
      "Step 100/2903, Loss: 92005.5703125\n",
      "Step 200/2903, Loss: 64004.5859375\n",
      "Step 300/2903, Loss: 88005.34375\n",
      "Step 400/2903, Loss: 73004.7109375\n",
      "Step 500/2903, Loss: 70004.15625\n",
      "Step 600/2903, Loss: 59004.1171875\n",
      "Step 700/2903, Loss: 81004.7421875\n",
      "Step 800/2903, Loss: 59004.55078125\n",
      "Step 900/2903, Loss: 72004.65625\n",
      "Step 1000/2903, Loss: 65004.734375\n",
      "Step 1100/2903, Loss: 47003.44140625\n",
      "Step 1200/2903, Loss: 83004.671875\n",
      "Step 1300/2903, Loss: 78005.359375\n",
      "Step 1400/2903, Loss: 77005.234375\n",
      "Step 1500/2903, Loss: 88005.328125\n",
      "Step 1600/2903, Loss: 81004.9296875\n",
      "Step 1700/2903, Loss: 63004.65625\n",
      "Step 1800/2903, Loss: 72004.734375\n",
      "Step 1900/2903, Loss: 68004.390625\n",
      "Step 2000/2903, Loss: 77005.015625\n",
      "Step 2100/2903, Loss: 81004.84375\n",
      "Step 2200/2903, Loss: 65004.53125\n",
      "Step 2300/2903, Loss: 69005.1015625\n",
      "Step 2400/2903, Loss: 84004.75\n",
      "Step 2500/2903, Loss: 86004.921875\n",
      "Step 2600/2903, Loss: 75004.65625\n",
      "Step 2700/2903, Loss: 73005.359375\n",
      "Step 2800/2903, Loss: 92005.234375\n",
      "Step 2900/2903, Loss: 67004.609375\n",
      "Epoch 5/5\n",
      "Step 0/2903, Loss: 77004.65625\n",
      "Step 100/2903, Loss: 97005.4453125\n",
      "Step 200/2903, Loss: 54004.37109375\n",
      "Step 300/2903, Loss: 84005.1328125\n",
      "Step 400/2903, Loss: 71004.515625\n",
      "Step 500/2903, Loss: 67003.875\n",
      "Step 600/2903, Loss: 56003.8984375\n",
      "Step 700/2903, Loss: 73004.390625\n",
      "Step 800/2903, Loss: 56004.34375\n",
      "Step 900/2903, Loss: 66004.4453125\n",
      "Step 1000/2903, Loss: 64004.56640625\n",
      "Step 1100/2903, Loss: 46003.4140625\n",
      "Step 1200/2903, Loss: 78004.546875\n",
      "Step 1300/2903, Loss: 75005.2109375\n",
      "Step 1400/2903, Loss: 73005.015625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import chess\n",
    "import chess.pgn\n",
    "from tensorflow.keras import models, layers, losses\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# Function to convert board state to one-hot encoding\n",
    "def board_to_one_hot(board):\n",
    "    one_hot = np.zeros((8, 8, 12), dtype=np.int8)\n",
    "    piece_map = {chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2,\n",
    "                 chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5}\n",
    "    for square in chess.scan_reversed(chess.BB_ALL):\n",
    "        piece = board.piece_at(square)\n",
    "        if piece is not None:\n",
    "            piece_index = piece_map[piece.piece_type] + (6 if piece.color else 0)\n",
    "            one_hot[chess.square_rank(square), chess.square_file(square), piece_index] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Function to convert move to label\n",
    "def move_to_label(move):\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    return from_square * 64 + to_square\n",
    "\n",
    "# Generator function to yield batches of data\n",
    "def data_generator(games, batch_size):\n",
    "    X_batch, y_batch, boards = [], [], []\n",
    "    while True:\n",
    "        for game in games:\n",
    "            board = game.board()\n",
    "            for move in game.mainline_moves():\n",
    "                X_batch.append(board_to_one_hot(board))\n",
    "                y_batch.append(move_to_label(move))\n",
    "                boards.append(board.copy())\n",
    "                board.push(move)\n",
    "                if len(X_batch) == batch_size:\n",
    "                    yield np.array(X_batch, dtype=np.float32), np.array(y_batch), boards\n",
    "                    X_batch, y_batch, boards = [], [], []\n",
    "\n",
    "# Define the CNN model\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(8, 8, 12), padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(4096, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Custom training loop\n",
    "def train_model(model, games, batch_size, epochs_per_cycle, steps_per_epoch):\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    illegal_move_penalty = 1000  # Large penalty for illegal moves\n",
    "\n",
    "    for epoch in range(epochs_per_cycle):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs_per_cycle}\")\n",
    "        for step, (X_batch, y_batch, boards) in enumerate(data_generator(games, batch_size)):\n",
    "            if step >= steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(X_batch, training=True)\n",
    "                loss = tf.keras.losses.categorical_crossentropy(to_categorical(y_batch, num_classes=4096), predictions)\n",
    "\n",
    "                # Penalty for illegal moves\n",
    "                for i, board in enumerate(boards):\n",
    "                    from_square, to_square = divmod(tf.argmax(predictions[i]).numpy(), 64)\n",
    "                    move = chess.Move(from_square, to_square)\n",
    "                    if move not in board.legal_moves:\n",
    "                        loss = loss + illegal_move_penalty\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step {step}/{steps_per_epoch}, Loss: {tf.reduce_mean(loss).numpy()}\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "games_per_batch = 5000\n",
    "epochs_per_cycle = 5  # Number of epochs to train on each batch of games\n",
    "\n",
    "# Train the model in cycles\n",
    "total_games = 50000\n",
    "model = create_model()\n",
    "\n",
    "for cycle in range(total_games // games_per_batch):\n",
    "    print(f\"Training cycle {cycle + 1}/{total_games // games_per_batch}\")\n",
    "    train_games = next(parse_pgn_file_in_batches(pgn_file, games_per_batch))\n",
    "    steps_per_epoch = sum(1 for game in train_games for _ in game.mainline_moves()) // batch_size\n",
    "    \n",
    "    train_model(model, train_games, batch_size, epochs_per_cycle, steps_per_epoch)\n",
    "\n",
    "# Evaluate the model using the last chunk of test games\n",
    "test_games = next(parse_pgn_file_in_batches(pgn_file, games_per_batch))\n",
    "validation_steps = sum(1 for game in test_games for _ in game.mainline_moves()) // batch_size\n",
    "\n",
    "def evaluate_model(model, games, batch_size, steps):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for step, (X_batch, y_batch, boards) in enumerate(data_generator(games, batch_size)):\n",
    "        if step >= steps:\n",
    "            break\n",
    "        predictions = model(X_batch, training=False)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(to_categorical(y_batch, num_classes=4096), predictions)\n",
    "        \n",
    "        # Check for illegal moves and penalize\n",
    "        for i, board in enumerate(boards):\n",
    "            from_square, to_square = divmod(tf.argmax(predictions[i]).numpy(), 64)\n",
    "            move = chess.Move(from_square, to_square)\n",
    "            if move not in board.legal_moves:\n",
    "                loss = loss + illegal_move_penalty\n",
    "\n",
    "        total_loss += tf.reduce_mean(loss).numpy()\n",
    "        total_acc += tf.reduce_mean(tf.keras.metrics.categorical_accuracy(to_categorical(y_batch, num_classes=4096), predictions)).numpy()\n",
    "\n",
    "    avg_loss = total_loss / steps\n",
    "    avg_acc = total_acc / steps\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "test_loss, test_acc = evaluate_model(model, test_games, batch_size, validation_steps)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a0439-94bd-4ad2-a0c0-34750bc07598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_weights_v5.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8b976-665f-4699-a44b-6085a6797490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
