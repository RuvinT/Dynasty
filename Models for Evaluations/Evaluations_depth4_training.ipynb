{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ3MUQOB9kQi",
        "outputId": "522e19cb-eba8-4647-e1a6-e4a3d8429b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-chess\n",
            "  Downloading python_chess-1.999-py3-none-any.whl (1.4 kB)\n",
            "Collecting chess<2,>=1 (from python-chess)\n",
            "  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chess, python-chess\n",
            "Successfully installed chess-1.10.0 python-chess-1.999\n"
          ]
        }
      ],
      "source": [
        "pip install python-chess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYgEZb7d9s75",
        "outputId": "495bfef6-1361-4e25-d3b3-067e041ed5b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import chess\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "csv_file = \"/content/drive/My Drive/evaluations_depth_4.csv\"\n",
        " #Disable XLA JIT compilation\n",
        "tf.config.optimizer.set_jit(False)\n",
        "\n",
        "# Set cuDNN autotune environment variable to avoid problematic algorithms\n",
        "import os\n",
        "os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'\n",
        "def create_model():\n",
        "    input_layer = Input(shape=(14, 8, 8))\n",
        "\n",
        "    conv_layer_1 = Conv2D(128, (3, 3), padding='same', activation='relu')(input_layer)\n",
        "    conv_layer_2 = Conv2D(128, (3, 3), padding='same', activation='relu')(conv_layer_1)\n",
        "    conv_layer_3 = Conv2D(128, (3, 3), padding='same', activation='relu')(conv_layer_2)\n",
        "\n",
        "\n",
        "    flatten_layer = Flatten()(conv_layer_2)\n",
        "\n",
        "    dense_layer_1 = Dense(128, activation='relu')(flatten_layer)\n",
        "    output_layer = Dense(1, activation='linear')(dense_layer_1)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(5e-4), loss=\"mean_squared_error\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "board_positions = {chr(i): i - ord('a') for i in range(ord('a'), ord('h') + 1)}\n",
        "\n",
        "def standardize_evaluation(value, min_value, max_value):\n",
        "    try:\n",
        "        value = float(value)\n",
        "    except ValueError:\n",
        "        cleaned_value = value.lstrip('#')\n",
        "        try:\n",
        "            numeric_value = float(cleaned_value)\n",
        "            value = numeric_value * 15000\n",
        "        except ValueError:\n",
        "            print(\"cannot clean\")\n",
        "    standardized_value = (value - min_value) / (max_value - min_value)\n",
        "    return standardized_value\n",
        "\n",
        "def square_to_index(square):\n",
        "    letter = chess.square_name(square)\n",
        "    row = 8 - int(letter[1])\n",
        "    column = board_positions[letter[0]]\n",
        "    return row, column\n",
        "\n",
        "def boards_to_matrices_batch(boards):\n",
        "    batch_size = len(boards)\n",
        "    board_3d_batch = np.zeros((batch_size, 14, 8, 8), dtype=np.int8)\n",
        "\n",
        "    for b_idx, board in enumerate(boards):\n",
        "        for piece in chess.PIECE_TYPES:\n",
        "            for square in board.pieces(piece, chess.WHITE):\n",
        "                index = np.unravel_index(square, (8, 8))\n",
        "                board_3d_batch[b_idx, piece - 1, 7 - index[0], index[1]] = 1\n",
        "            for square in board.pieces(piece, chess.BLACK):\n",
        "                index = np.unravel_index(square, (8, 8))\n",
        "                board_3d_batch[b_idx, piece + 5, 7 - index[0], index[1]] = 1\n",
        "\n",
        "        aux = board.turn\n",
        "        board.turn = chess.WHITE\n",
        "        for move in board.legal_moves:\n",
        "            i, j = np.unravel_index(move.to_square, (8, 8))\n",
        "            board_3d_batch[b_idx, 12, 7 - i, j] = 1\n",
        "\n",
        "        board.turn = chess.BLACK\n",
        "        for move in board.legal_moves:\n",
        "            i, j = np.unravel_index(move.to_square, (8, 8))\n",
        "            board_3d_batch[b_idx, 13, 7 - i, j] = 1\n",
        "\n",
        "        board.turn = aux\n",
        "\n",
        "    return board_3d_batch\n",
        "\n",
        "def process_batch(batch):\n",
        "    valid_rows = batch.dropna(subset=['fen', 'evaluation'])\n",
        "    y_evaluation = valid_rows['evaluation'].apply(standardize_evaluation, args=(-10000, 10000)).values\n",
        "\n",
        "    feature_boards = []\n",
        "    for fen in valid_rows['fen']:\n",
        "        board = chess.Board(fen)\n",
        "        feature_boards.append(board)\n",
        "    feature_board = boards_to_matrices_batch(feature_boards)\n",
        "    return feature_board, y_evaluation\n",
        "\n",
        "def data_generator_from_csv(csv_file, batch_size):\n",
        "    chunk_size = batch_size\n",
        "    while True:\n",
        "        for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
        "            processed_data = process_batch(chunk)\n",
        "            if processed_data:\n",
        "                x_data, y_data = processed_data\n",
        "                yield x_data, y_data\n",
        "\n",
        "def tf_data_generator(csv_file, batch_size):\n",
        "    generator = lambda: data_generator_from_csv(csv_file, batch_size)\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, 14, 8, 8), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 512  # Adjust batch size if necessary\n",
        "train_dataset = tf_data_generator(csv_file, batch_size)\n",
        "\n",
        "model = create_model()\n",
        "epochs = 100  # One epoch for each batch\n",
        "\n",
        "num_samples = sum(1 for row in open(csv_file)) - 1  # Subtract 1 for header\n",
        "steps_per_epoch = (num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='/content/drive/My Drive/models_save/model_checkpoint_depth4_{epoch:02d}.h5',  # Filename pattern to save the model\n",
        "    save_freq='epoch',  # Save the model at the end of every epoch\n",
        "    save_best_only=False  # You can set this to True to save only the best model\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    callbacks=[\n",
        "        ReduceLROnPlateau(monitor=\"loss\", patience=10),\n",
        "        EarlyStopping(monitor=\"loss\", patience=15, min_delta=1e-4),\n",
        "        checkpoint_callback\n",
        "    ]\n",
        ")\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf2hwRHd9tXW",
        "outputId": "8b58300c-ee68-4dd0-ca56-309558bc1388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 14, 8, 8)]        0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 14, 8, 128)        9344      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 8, 128)        147584    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 14336)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1835136   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1992193 (7.60 MB)\n",
            "Trainable params: 1992193 (7.60 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "9524/9524 [==============================] - 17025s 2s/step - loss: 4.5876e-04 - lr: 5.0000e-04\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9524/9524 [==============================] - 17033s 2s/step - loss: 3.5657e-05 - lr: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100\n",
            "5046/9524 [==============>...............] - ETA: 2:13:57 - loss: 1.7848e-05"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EmNmAtfakk5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}